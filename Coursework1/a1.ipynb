{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 1\n",
    "\n",
    "For this assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about these coursework questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateno=184514 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bayley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bayley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bayley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "sys.path.append(r'/Users/Bayley/Documents/resources')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.classify.api import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Naive Bayes Classification (25 marks)\n",
    "\n",
    "In this question, you will be considering how a Naive Bayes classifier can be applied to the task of deciding whether sentences are relevant or not relevant to the kitchen domain.\n",
    "\n",
    "The code below will generate for you two small unique sets of sentences, which you should refer to in your answer to this question.   This question will be marked on the quality of your explanations rather than the quality of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences is 49.  Number of testing sentences is 12\n"
     ]
    }
   ],
   "source": [
    "#Do NOT change the code in this cell.\n",
    "\n",
    "topics=[\"book\",\"kitchen\",\"dvd\",\"electronics\"]\n",
    "samplesize=20\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "trainingsentences=[]\n",
    "testsentences=[]\n",
    "cr = AmazonReviewCorpusReader()\n",
    "\n",
    "for topic in topics:\n",
    "    random.seed(candidateno)\n",
    "    if topic == \"kitchen\":\n",
    "        key=\"kitchen\"\n",
    "    else:\n",
    "        key=\"not-kitchen\"\n",
    "    topicsentences=[]\n",
    "    while len(topicsentences)<10:\n",
    "        topicsentences=[({token.lower():True for token in doc if token not in stop and token.isalpha()},key) for doc in cr.category(topic).sample_sents(samplesize=samplesize) if len(doc)>0]\n",
    "    testsentences+=topicsentences[0:3]\n",
    "    trainingsentences+=topicsentences[3:]\n",
    "\n",
    "random.shuffle(trainingsentences)\n",
    "mixup=testsentences[:4]+testsentences[5:]\n",
    "random.shuffle(mixup)\n",
    "testsentences=[testsentences[4]]+mixup\n",
    "print(\"Number of training sentences is {}.  Number of testing sentences is {}\".format(len(trainingsentences),len(testsentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prior_prob(training_data):\n",
    "    kitchen_count = 0\n",
    "    not_kitchen_count = 0\n",
    "    dict = {}\n",
    "    for d in training_data:\n",
    "        if(d[1] == 'kitchen'):\n",
    "            kitchen_count += 1\n",
    "        if(d[1] == 'not-kitchen'):\n",
    "            not_kitchen_count +=  1\n",
    "    dict['kitchen'] = kitchen_count / len(training_data)\n",
    "    dict['not-kitchen'] = not_kitchen_count / len(training_data)\n",
    "    total = kitchen_count + not_kitchen_count\n",
    "    print(\"Number of kitchen documents:\",str(kitchen_count))\n",
    "    print(\"Number of non-kitchen documents:\",str(not_kitchen_count))\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_known(training_data):\n",
    "    words_known = set()\n",
    "    for phrase in training_data:\n",
    "        for word in phrase[0]:\n",
    "            words_known.add(word)\n",
    "    \n",
    "    return words_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(training_data):\n",
    "    kitchen_count = {}\n",
    "    not_kitchen_count = {}\n",
    "    result = {}\n",
    "    i = 0 #counter\n",
    "    vocabulary_known = list(vocab_known(training_data))\n",
    "    for data in training_data:\n",
    "        if data[1] == 'kitchen':\n",
    "            for phrase in data[0]:\n",
    "                for word in phrase.split():\n",
    "                    kitchen_count[word] = (kitchen_count.get(word,0)+1)\n",
    "                    i += 1\n",
    "    for word in vocabulary_known:\n",
    "        if word in kitchen_count.keys():\n",
    "            i += 1\n",
    "            kitchen_count[word] += 1\n",
    "        else:\n",
    "            kitchen_count[word] = 1\n",
    "            i += 1\n",
    "    for key,value in kitchen_count.items():\n",
    "        kitchen_count[key] = (value / i)\n",
    "    print(kitchen_count)\n",
    "    print(i)\n",
    "    \n",
    "    i = 0\n",
    "    for data in training_data:\n",
    "        if data[1] == 'not-kitchen':\n",
    "            for phrase in data[0]:\n",
    "                for word in phrase.split():\n",
    "                    not_kitchen_count[word] = (not_kitchen_count.get(word,0)+1)\n",
    "                    i += 1\n",
    "    for word in vocabulary_known:\n",
    "        if word in not_kitchen_count.keys():\n",
    "            i += 1\n",
    "            not_kitchen_count[word] += 1\n",
    "        else:\n",
    "            not_kitchen_count[word] = 1\n",
    "            i += 1\n",
    "    for key,value in not_kitchen_count.items():\n",
    "        not_kitchen_count[key] = (value / i)\n",
    "    \n",
    "    result['not-kitchen \\n'] = not_kitchen_count\n",
    "    result['kitchen \\n'] = kitchen_count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(d, priors, c_probs):\n",
    "    label=\"\"\n",
    "    probs_kitchen = 0\n",
    "    probs_not_kitchen = 0\n",
    "    words_known = vocab_known(training_data)\n",
    "    for word in d:\n",
    "        if word[0] in words_known:\n",
    "            for word in d:\n",
    "                if word[0] in c_probs['kitchen'].keys():\n",
    "                    probs_kitchen =+ math.log10(c_probs['kitchen'][word[0]])\n",
    "            for word in d:\n",
    "                if word[0] in c_probs['not-kitchen'].keys():\n",
    "                    probs_kitchen =+ math.log10(c_probs['not-kitchen'][word[0]])\n",
    "                    \n",
    "            probs_kitchen =+ math.log10(priors['kitchen'])\n",
    "            probs_not_kitchen =+ math.log10(priors['not-kitchen'])\n",
    "            \n",
    "            if probs_kitchen > probs_not_kitchen:\n",
    "                label = \"kitchen\"\n",
    "            elif probs_kitchen < probs_not_kitchen:\n",
    "                label = \"not-kitchen\"\n",
    "            else:\n",
    "                label = random.choice(['kitchen'],['not-kitchen'])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "class NBClassifier(ClassifierI):\n",
    "    def __init__(self):\n",
    "        self.labels=[\"P\",\"N\"]\n",
    "        pass\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    \n",
    "    def _set_known_vocabulary(self,training_data):\n",
    "        known = []\n",
    "        \n",
    "        for (d, label) in training_data:\n",
    "            known+=list(d.keys())\n",
    "            \n",
    "        self.known = set(known)\n",
    "    def _set_priors(self,training_data):\n",
    "        priors_probs={}\n",
    "        \n",
    "        for (d, label) in training_data:\n",
    "            priors_probs[label] = priors_probs.get(label,0) + 1 #smoothing\n",
    "        total=sum(priors_probs.values())\n",
    "        \n",
    "        for (key,value) in priors_probs.items():\n",
    "            priors_probs[key]=value/total\n",
    "            \n",
    "        self.priors_probs=priors_probs\n",
    "    def _set_cond_probs(self, training_data):\n",
    "        cond_probs={}\n",
    "        \n",
    "        for (d, label) in training_data:\n",
    "            class_cond = cond_probs.get(label,{})\n",
    "            \n",
    "            for word in d.keys():\n",
    "                class_cond[word] = class_cond.get(word,0) + 1 #smoothing\n",
    "            \n",
    "            cond_probs[label] = class_cond\n",
    "    \n",
    "        for (label, class_cond) in cond_probs.items():\n",
    "            \n",
    "            for word in self.known:\n",
    "                class_cond[word] = class_cond.get(word,0) + 1\n",
    "            cond_probs[label] = class_cond\n",
    "            \n",
    "        for (label, distance) in cond_probs.items():\n",
    "            total = sum(distance.values())\n",
    "            cond_probs[label] = {key:value/total for (key, value) in distance.items()}\n",
    "        \n",
    "        self.cond_probs = cond_probs\n",
    "    def train(self, training_data):\n",
    "        self._set_known_vocabulary(training_data)\n",
    "        self._set_priors(training_data)\n",
    "        self._set_cond_probs(training_data)\n",
    "    \n",
    "    def classify(self, d):\n",
    "        d_probs = {key:math.log(value) for (key,value) in self.priors_probs.items()}\n",
    "        for word in d[0]:\n",
    "            if word in self.known:\n",
    "                d_probs = {classlabel:sofar+math.log(self.cond_probs[classlabel].get(word,0)) for (classlabel,sofar) in d_probs.items()}\n",
    "                \n",
    "        highprobs = max(d_probs.values())\n",
    "        classes = [c for c in d_probs.keys() if d_probs[c] == highprobs]\n",
    "        return random.choice(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) With reference to the sentences generated above, **explain** how a Naive Bayes classifer would be trained to carry out the task of deciding whether sentences are relevant to the kitchen domain.  You do **not** need to build or train a classifier.  However, you should explain the relevant probabilities with reference to examples taken from your samples of sentences.  \\[10 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions Made\n",
    "\n",
    "1. Each document is treated as a **bag-of-words**, meaning that the word order and grammatical structure is ignored. Preprocessing here is critical, as it helps constitute what a word is. Through **feature extraction**, words are treated as features, so words within documents provide evidence of being in a certain class. \n",
    "\n",
    "2. Use of **multi-variate Bernouilli event model**.This is the frequency of a word in a document, isn't considered, just whether a word occurs or not (*binary-valued* representation checking if word occurs). Training documents will be presented as a pair, a dictionary that maps each word that appears to True, and a string denoting the class of the document.\n",
    "\n",
    "### How the classifier works\n",
    "\n",
    "To train data, is to manually specifying which class the sentence belongs to. The AmazonCorpus specifies what class the sentence belongs to, and is used to create the training/testing lists. Training sentences will be given either the class 'Kitchen' or 'Not Kitchen', and these will be compared to the testing sentences to decide what class the testing sentences belong to. In order to classify a document, $d$, we need to determine which of the probabilities is greater:\n",
    "\n",
    "$$P(\\,\\mbox{kitchen}\\,|\\,d) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{not kitchen}\\,|\\,d)$$\n",
    "\n",
    "where $d$ is the document.\n",
    "\n",
    "$P(X|Y)$ means the probability of $X$ given $Y$. So, $P(\\,\\mbox{kitchen}\\,|\\,d)$ means the probability, given a document $d$, of $d$ being of class `kitchen`.\n",
    "To calculate the probabilities, Bayes' rule can be applied, leading to the following comparison:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{kitchen}\\,)\\cdot P(\\,\\mbox{kitchen}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{not kitchen}\\,)\\cdot P(\\,\\mbox{not kitchen}\\,)$$\n",
    "\n",
    "Here $P(\\,d\\,|\\,\\mbox{kitchen}\\,)$  is the probability of a document in the `kitchen` category being the document $d$ and, $P(\\,\\mbox{kitchen}\\,)$ is the probability of a randomly selected document being of category `kitchen`.\n",
    "\n",
    "### Class Priors\n",
    "\n",
    "As stated , $P(\\,\\mbox{kitchen}\\,)$ is the probability of a randomly selected document being of category `kitchen`, where as $P(\\,\\mbox{not kitchen}\\,)$ is the probability of a randomly selected document being of category `not kitchen`. This is called the class priors. We can obtain estimated values for these probabilities using the training data. If the training data consisted of $n_1$ documents of class `kitchen` and $n_2$ documents of class `not kitchen`, then we can represent the probabilities as:\n",
    "\n",
    "$$P(\\mbox{Kitchen})=\\frac{n_1}{n_1+n_2} \\qquad \\mbox{and} \\qquad P(\\mbox{not Kitchen}) = \\frac{n_2}{n_1+n_2}$$\n",
    "Where $n_1$ is the number of documents in the training documents classed as `Kitchen` and $n_2$ documents of class `not Kitchen`.\n",
    "The code below computes the number of documents in the training data that relate to each class, `kitchen` and `not-kitchen`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences:\n",
      "Number of kitchen documents: 12\n",
      "Number of non-kitchen documents: 37\n",
      "Testing sentences:\n",
      "Number of kitchen documents: 3\n",
      "Number of non-kitchen documents: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Training sentences:\")\n",
    "train_prior_probs = class_prior_prob(trainingsentences)\n",
    "print(\"Testing sentences:\")\n",
    "test_prior_probs = class_prior_prob(testsentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training data above, `P(kitchen)` will have a prior probability of 12 / 12 + 37, which equals:\n",
    "\n",
    "$$P(\\mbox{kitchen})=\\frac{12}{49} \\qquad \\mbox{≃} \\qquad \\mbox{0.24 (2.d.p)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `P(not kitchen)` will have a prior probability of 37 / 12 + 37 which equals:\n",
    "\n",
    "$$P(\\mbox{not kitchen})=\\frac{37}{49} \\qquad \\mbox{≃} \\qquad \\mbox{0.76 (2.d.p)}$$\n",
    "\n",
    "The code below computes estimates of the class priors from the training data, which is expressed in the table below which prints the prior probability of the documents in the training data for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Prior Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.244898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>not-kitchen</td>\n",
       "      <td>0.755102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class  Prior Probability\n",
       "0      kitchen           0.244898\n",
       "1  not-kitchen           0.755102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_prior_probs.items(),columns=['Class','Prior Probability'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below the code, shows the class priors for the test sentnces given. As we know from above, there are 3 kitchen documents and 9 not-kicthen documents. This gives a total of 12 documents:\n",
    "\n",
    "$$P(\\mbox{kitchen})=\\frac{9}{12} \\qquad \\mbox{=} \\qquad \\mbox{0.25 (2.d.p)} \\qquad \\mbox{and} \\qquad P(\\mbox{not-kitchen})=\\frac{9}{12} \\qquad \\mbox{=} \\qquad \\mbox{0.75 (2.d.p)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Prior Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>not-kitchen</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class  Prior Probability\n",
       "0      kitchen               0.25\n",
       "1  not-kitchen               0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_prior_probs.items(),columns=['Class','Prior Probability'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities\n",
    "\n",
    "For each feature, we wish to know the probability of occurrence for each class:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{kitchen}\\,)$$  $$P(\\,d\\,|\\,\\mbox{not kitchen}\\,)$$\n",
    "\n",
    "In order to **estimate** the probability, we know the label of each document **label(di)** and features of each document **feats(di)**. The problem we have is that $d$ is a document, that we won't have seen in the training data. \n",
    "The code below computes estimates of the conditional probability of a word given a class from training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0.01642710472279261, 'believe': 0.004106776180698152, 'bacteria': 0.004106776180698152, 'growth': 0.004106776180698152, 'answer': 0.004106776180698152, 'making': 0.004106776180698152, 'espresso': 0.004106776180698152, 'easy': 0.004106776180698152, 'enough': 0.004106776180698152, 'follow': 0.004106776180698152, 'instructions': 0.004106776180698152, 'watch': 0.004106776180698152, 'video': 0.004106776180698152, 'think': 0.004106776180698152, 'well': 0.004106776180698152, 'insulated': 0.004106776180698152, 'turn': 0.004106776180698152, 'water': 0.004106776180698152, 'cools': 0.004106776180698152, 'faster': 0.004106776180698152, 'would': 0.004106776180698152, 'ca': 0.004106776180698152, 'say': 0.004106776180698152, 'really': 0.004106776180698152, 'drawback': 0.004106776180698152, 'adding': 0.004106776180698152, 'porcelain': 0.004106776180698152, 'enamel': 0.004106776180698152, 'looks': 0.004106776180698152, 'good': 0.004106776180698152, 'multiple': 0.004106776180698152, 'benefits': 0.006160164271047228, 'love': 0.004106776180698152, 'utensils': 0.004106776180698152, 'nice': 0.004106776180698152, 'command': 0.004106776180698152, 'hand': 0.004106776180698152, 'the': 0.008213552361396304, 'cooking': 0.004106776180698152, 'le': 0.004106776180698152, 'creuset': 0.004106776180698152, 'cast': 0.004106776180698152, 'iron': 0.004106776180698152, 'many': 0.004106776180698152, 'lets': 0.004106776180698152, 'get': 0.004106776180698152, 'buffet': 0.004106776180698152, 'casserole': 0.004106776180698152, 'specifically': 0.004106776180698152, 'cool': 0.004106776180698152, 'idea': 0.004106776180698152, 'maker': 0.004106776180698152, 'mention': 0.004106776180698152, 'operate': 0.004106776180698152, 'flimsy': 0.004106776180698152, 'plastic': 0.004106776180698152, 'button': 0.004106776180698152, 'allow': 0.004106776180698152, 'nozzle': 0.004106776180698152, 'open': 0.004106776180698152, 'pivot': 0.004106776180698152, 'may': 0.004106776180698152, 'include': 0.004106776180698152, 'every': 0.006160164271047228, 'time': 0.006160164271047228, 'use': 0.006160164271047228, 'since': 0.004106776180698152, 'stores': 0.004106776180698152, 'best': 0.008213552361396304, 'closed': 0.004106776180698152, 'unusable': 0.004106776180698152, 'position': 0.004106776180698152, 'if': 0.004106776180698152, 'totally': 0.004106776180698152, 'field': 0.004106776180698152, 'strip': 0.004106776180698152, 'clean': 0.004106776180698152, 'mediocre': 0.004106776180698152, 'little': 0.004106776180698152, 'shark': 0.004106776180698152, 'came': 0.004106776180698152, 'free': 0.004106776180698152, 'big': 0.004106776180698152, 'one': 0.006160164271047228, 'overpriced': 0.004106776180698152, 'looked': 0.004106776180698152, 'lot': 0.004106776180698152, 'deli': 0.004106776180698152, 'slicers': 0.004106776180698152, 'compared': 0.004106776180698152, 'features': 0.004106776180698152, 'prices': 0.004106776180698152, 'difficulties': 0.004106776180698152, 'even': 0.004106776180698152, 'though': 0.004106776180698152, 'newest': 0.004106776180698152, 'model': 0.004106776180698152, 'used': 0.004106776180698152, 'dishwasher': 0.002053388090349076, 'mtv': 0.002053388090349076, 'gone': 0.002053388090349076, 'exposure': 0.002053388090349076, 'parents': 0.002053388090349076, 'second': 0.002053388090349076, 'reality': 0.002053388090349076, 'incredible': 0.002053388090349076, 'sexual': 0.002053388090349076, 'still': 0.002053388090349076, 'laptop': 0.002053388090349076, 'epson': 0.002053388090349076, 'morning': 0.002053388090349076, 'printing': 0.002053388090349076, 'tv': 0.002053388090349076, 'invoked': 0.002053388090349076, 'noise': 0.002053388090349076, 'series': 0.002053388090349076, 'rivaled': 0.002053388090349076, 'do': 0.002053388090349076, 'scenes': 0.002053388090349076, 'rozsa': 0.002053388090349076, 'cartridge': 0.002053388090349076, 'neverending': 0.002053388090349076, 'claim': 0.002053388090349076, 'us': 0.002053388090349076, 'continues': 0.002053388090349076, 'young': 0.002053388090349076, 'numbers': 0.002053388090349076, 'worker': 0.002053388090349076, 'dimension': 0.002053388090349076, 'dangerous': 0.002053388090349076, 'smaller': 0.002053388090349076, 'god': 0.002053388090349076, 'going': 0.002053388090349076, 'somehow': 0.002053388090349076, 'to': 0.002053388090349076, 'meet': 0.002053388090349076, 'sam': 0.002053388090349076, 'miklos': 0.002053388090349076, 'overused': 0.002053388090349076, 'go': 0.002053388090349076, 'skype': 0.002053388090349076, 'a': 0.002053388090349076, 'awesome': 0.002053388090349076, 'first': 0.002053388090349076, 'preston': 0.002053388090349076, 'five': 0.002053388090349076, 'insurance': 0.002053388090349076, 'weeks': 0.002053388090349076, 'plot': 0.002053388090349076, 'older': 0.002053388090349076, 'art': 0.002053388090349076, 'horse': 0.002053388090349076, 'perhaps': 0.002053388090349076, 'films': 0.002053388090349076, 'brothers': 0.002053388090349076, 'advised': 0.002053388090349076, 'plotlines': 0.002053388090349076, 'several': 0.002053388090349076, 'bought': 0.002053388090349076, 'twists': 0.002053388090349076, 'brother': 0.002053388090349076, 'author': 0.002053388090349076, 'mind': 0.002053388090349076, 'accomodate': 0.002053388090349076, 'truth': 0.002053388090349076, 'sony': 0.002053388090349076, 'wage': 0.002053388090349076, 'life': 0.002053388090349076, 'value': 0.002053388090349076, 'come': 0.002053388090349076, 'whichever': 0.002053388090349076, 'great': 0.002053388090349076, 'tries': 0.002053388090349076, 'sleep': 0.002053388090349076, 'reminds': 0.002053388090349076, 'views': 0.002053388090349076, 'moral': 0.002053388090349076, 'also': 0.002053388090349076, 'seen': 0.002053388090349076, 'book': 0.002053388090349076, 'suffer': 0.002053388090349076, 'long': 0.002053388090349076, 'running': 0.002053388090349076, 'relationship': 0.002053388090349076, 'subtract': 0.002053388090349076, 'amazon': 0.002053388090349076, 'option': 0.002053388090349076, 'wo': 0.002053388090349076, 'scratching': 0.002053388090349076, 'call': 0.002053388090349076, 'ramanujun': 0.002053388090349076, 'especially': 0.002053388090349076, 'lion': 0.002053388090349076, 'and': 0.002053388090349076, 'house': 0.002053388090349076, 'least': 0.002053388090349076, 'expansion': 0.002053388090349076, 'vons': 0.002053388090349076, 'crush': 0.002053388090349076, 'want': 0.002053388090349076, 'could': 0.002053388090349076, 'users': 0.002053388090349076, 'money': 0.002053388090349076, 'wrote': 0.002053388090349076, 'compromising': 0.002053388090349076, 'destroyed': 0.002053388090349076, 'someone': 0.002053388090349076, 'emotions': 0.002053388090349076, 'move': 0.002053388090349076, 'bomb': 0.002053388090349076, 'story': 0.002053388090349076, 'add': 0.002053388090349076, 'know': 0.002053388090349076, 'dvd': 0.002053388090349076, 'travel': 0.002053388090349076, 'patients': 0.002053388090349076, 'look': 0.002053388090349076, 'reinstalling': 0.002053388090349076, 'able': 0.002053388090349076, 'disney': 0.002053388090349076, 'lack': 0.002053388090349076, 'narrates': 0.002053388090349076, 'irish': 0.002053388090349076, 'movie': 0.002053388090349076, 'cross': 0.002053388090349076, 'offered': 0.002053388090349076, 'chances': 0.002053388090349076, 'but': 0.002053388090349076, 'turmoil': 0.002053388090349076, 'important': 0.002053388090349076, 'psychedelics': 0.002053388090349076, 'head': 0.002053388090349076, 'music': 0.002053388090349076, 'city': 0.002053388090349076, 'researchers': 0.002053388090349076, 'started': 0.002053388090349076, 'labyrinth': 0.002053388090349076, 'rhetoric': 0.002053388090349076, 'living': 0.002053388090349076, 'quality': 0.002053388090349076, 'got': 0.002053388090349076, 'crappy': 0.002053388090349076, 'real': 0.002053388090349076, 'you': 0.002053388090349076, 'score': 0.002053388090349076, 'gems': 0.002053388090349076, 'question': 0.002053388090349076, 'help': 0.002053388090349076, 'mcknights': 0.002053388090349076, 'showcasing': 0.002053388090349076, 'care': 0.002053388090349076, 'seton': 0.002053388090349076, 'entrances': 0.002053388090349076, 'stolen': 0.002053388090349076, 'academics': 0.002053388090349076, 'work': 0.002053388090349076, 'operation': 0.002053388090349076, 'suppose': 0.002053388090349076, 'naked': 0.002053388090349076, 'currently': 0.002053388090349076, 'there': 0.002053388090349076, 'fine': 0.002053388090349076, 'without': 0.002053388090349076, 'much': 0.002053388090349076, 'king': 0.002053388090349076, 'reading': 0.002053388090349076, 'see': 0.002053388090349076, 'thank': 0.002053388090349076, 'peckinpah': 0.002053388090349076, 'america': 0.002053388090349076, 'filmmaking': 0.002053388090349076, 'advocated': 0.002053388090349076, 'step': 0.002053388090349076, 'form': 0.002053388090349076, 'issues': 0.002053388090349076, 'need': 0.002053388090349076, 'celtic': 0.002053388090349076, 'argues': 0.002053388090349076, 'unlike': 0.002053388090349076, 'like': 0.002053388090349076, 'lost': 0.002053388090349076, 'heyday': 0.002053388090349076, 'feature': 0.002053388090349076, 'feet': 0.002053388090349076, 'wonderful': 0.002053388090349076, 'he': 0.002053388090349076, 'resonate': 0.002053388090349076, 'female': 0.002053388090349076, 'uninstall': 0.002053388090349076, 'powerful': 0.002053388090349076, 'local': 0.002053388090349076, 'this': 0.002053388090349076, 'entitled': 0.002053388090349076, 'louie': 0.002053388090349076, 'short': 0.002053388090349076, 'war': 0.002053388090349076, 'receiver': 0.002053388090349076, 'selected': 0.002053388090349076, 'saw': 0.002053388090349076, 'log': 0.002053388090349076, 'health': 0.002053388090349076, 'people': 0.002053388090349076, 'is': 0.002053388090349076, 'caring': 0.002053388090349076, 'camera': 0.002053388090349076, 'conveniently': 0.002053388090349076, 'emotional': 0.002053388090349076, 'anything': 0.002053388090349076, 'they': 0.002053388090349076, 'r': 0.002053388090349076, 'before': 0.002053388090349076, 'ease': 0.002053388090349076, 'expensive': 0.002053388090349076, 'louise': 0.002053388090349076, 'provides': 0.002053388090349076, 'product': 0.002053388090349076, 'made': 0.002053388090349076, 'heaven': 0.002053388090349076, 'try': 0.002053388090349076, 'castmember': 0.002053388090349076, 'dell': 0.002053388090349076, 'day': 0.002053388090349076, 'wide': 0.002053388090349076, 'healthcare': 0.002053388090349076, 'put': 0.002053388090349076, 'quick': 0.002053388090349076, 'toys': 0.002053388090349076, 'next': 0.002053388090349076, 'bag': 0.002053388090349076, 'flames': 0.002053388090349076, 'career': 0.002053388090349076, 'around': 0.002053388090349076, 'case': 0.002053388090349076, 'programming': 0.002053388090349076, 'provide': 0.002053388090349076, 'upstairs': 0.002053388090349076, 'ever': 0.002053388090349076, 'squirming': 0.002053388090349076, 'of': 0.002053388090349076, 'software': 0.002053388090349076, 'indeed': 0.002053388090349076, 'it': 0.002053388090349076, 'battle': 0.002053388090349076, 'ask': 0.002053388090349076, 'wrestle': 0.002053388090349076, 'far': 0.002053388090349076, 'liked': 0.002053388090349076, 'from': 0.002053388090349076, 'programs': 0.002053388090349076, 'terrifying': 0.002053388090349076, 'etc': 0.002053388090349076, 'boy': 0.002053388090349076, 'giver': 0.002053388090349076, 'annually': 0.002053388090349076, 'track': 0.002053388090349076, 'serious': 0.002053388090349076, 'nothing': 0.002053388090349076, 'employers': 0.002053388090349076, 'mixed': 0.002053388090349076, 'thhinking': 0.002053388090349076, 'horowitz': 0.002053388090349076, 'printers': 0.002053388090349076, 'an': 0.002053388090349076, 'two': 0.002053388090349076, 'warner': 0.002053388090349076, 'sure': 0.002053388090349076, 'genius': 0.002053388090349076, 'certain': 0.002053388090349076, 'release': 0.002053388090349076, 'glimpses': 0.002053388090349076, 'videos': 0.002053388090349076, 'companies': 0.002053388090349076, 'anya': 0.002053388090349076, 'film': 0.002053388090349076}\n",
      "487\n",
      "{'it': 0.01639344262295082, 'really': 0.01639344262295082, 'ergonomically': 0.01639344262295082, 'designed': 0.01639344262295082, 'using': 0.01639344262295082, 'apple': 0.01639344262295082, 'slicer': 0.01639344262295082, 'best': 0.01639344262295082, 'way': 0.01639344262295082, 'eat': 0.01639344262295082, 'the': 0.01639344262295082, 'diadem': 0.01639344262295082, 'cooking': 0.01639344262295082, 'set': 0.01639344262295082, 'roughly': 0.01639344262295082, 'meets': 0.01639344262295082, 'wmf': 0.01639344262295082, 'criteria': 0.01639344262295082, 'though': 0.01639344262295082, 'slight': 0.01639344262295082, 'imperfections': 0.01639344262295082, 'metal': 0.01639344262295082, 'top': 0.01639344262295082, 'survival': 0.00819672131147541, 'stop': 0.00819672131147541, 'acomplished': 0.00819672131147541, 'starting': 0.00819672131147541, 'powers': 0.00819672131147541, 'enthusiast': 0.00819672131147541, 'new': 0.00819672131147541, 'getting': 0.00819672131147541, 'demonstrates': 0.00819672131147541, 'common': 0.00819672131147541, 'quality': 0.00819672131147541, 'companions': 0.00819672131147541, 'learn': 0.00819672131147541, 'in': 0.00819672131147541, 'sale': 0.00819672131147541, 'made': 0.00819672131147541, 'start': 0.00819672131147541, 'support': 0.00819672131147541, 'called': 0.00819672131147541, 'spacebar': 0.00819672131147541, 'looks': 0.00819672131147541, 'fairly': 0.00819672131147541, 'superior': 0.00819672131147541, 'scenes': 0.00819672131147541, 'known': 0.00819672131147541, 'ziglars': 0.00819672131147541, 'realistic': 0.00819672131147541, 'secrets': 0.00819672131147541, 'beauty': 0.00819672131147541, 'scenic': 0.00819672131147541, 'parts': 0.00819672131147541, 'i': 0.00819672131147541, 'keeping': 0.00819672131147541, 'quite': 0.00819672131147541, 'much': 0.00819672131147541, 'journey': 0.00819672131147541, 'actually': 0.00819672131147541, 'sales': 0.00819672131147541, 'see': 0.00819672131147541, 'mystic': 0.00819672131147541, 'growing': 0.00819672131147541, 'money': 0.00819672131147541, 'desert': 0.00819672131147541, 'said': 0.00819672131147541, 'shot': 0.00819672131147541, 'that': 0.00819672131147541, 'spending': 0.00819672131147541, 'laugh': 0.00819672131147541, 'compared': 0.00819672131147541, 'little': 0.00819672131147541, 'skills': 0.00819672131147541, 'unlike': 0.00819672131147541, 'zig': 0.00819672131147541, 'silent': 0.00819672131147541, 'xhabbo': 0.00819672131147541, 'reviews': 0.00819672131147541, 'customer': 0.00819672131147541, 'books': 0.00819672131147541, 'as': 0.00819672131147541, 'comment': 0.00819672131147541, 'blend': 0.00819672131147541, 'decent': 0.00819672131147541, 'movie': 0.00819672131147541, 'almost': 0.00819672131147541, 'alaska': 0.00819672131147541, 'customers': 0.00819672131147541, 'model': 0.00819672131147541, 'master': 0.00819672131147541, 'two': 0.00819672131147541, 'equal': 0.00819672131147541, 'closing': 0.00819672131147541, 'challenges': 0.00819672131147541, 'better': 0.00819672131147541, 'existing': 0.00819672131147541, 'sound': 0.00819672131147541, 'film': 0.00819672131147541}\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "train_cond_probs = conditional_prob(trainingsentences)\n",
    "test_cond_probs = conditional_prob(testsentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naïve Bayes model of document classification addresses this by making a major simplifying assumption, that the probability that different words occur in a document are independent of one another. An example shown below:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,\\mbox{\"Isolation from outside\"}\\,|\\,\\mbox{kitchen}\\,) &=& P(\\{\\mbox{\"Isolation\"},\\mbox{\"from\"},\\mbox{\"outside\"},\\}\\,|\\,\\mbox{kitchen}\\,)\\\\\n",
    "&=& P(\\,\\mbox{kitchen}\\,)\\times P(\\mbox{\"Isolation\"}\\,|\\,\\mbox{kitchen}\\,)\\times P(\\mbox{\"from\"}\\,|\\,\\mbox{kitchen}\\,)\\times P(\\mbox{\"outside\"}\\,|\\,\\mbox{kitchen}\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "To estimate conditional probabilities, we take the number of times words occur in `kitchen` along with all words that are defined as the class `kitchen`. The probability that a word occurs in the classis the number of times it occurs divided by the number of words defined as part of the class **kitchen**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) With reference to the sentences generated above, **explain** how a trained Naive Bayes classifier would assign a class to a sentence. \\[5 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify a sentence, the probability of the sentence being in the `kitchen` class given the sentence is compared to the probability it is in the `not-kitchen` class. The higher probability , determines the  class given. \n",
    "\n",
    "If a sentence has equal probability of being either class, for example:\n",
    "\n",
    "$$P(\\,\\mbox{kitchen}) \\qquad\\qquad \\mbox{=} \\qquad\\qquad P(\\,\\mbox{not kitchen})$$\n",
    "\n",
    "Then the classifier would randomly choose one of the classes using a random function, as the sentence is no more one class than another.\n",
    "\n",
    "\n",
    "### Add one smoothing\n",
    "\n",
    "Some words many occur in the `kitchen` class, but may not occur at all in the `not-kitchen` class. This leads to an issue when calculating the conditional probabilities for documents. By not occurring in the `not kitchen` , the probability  would be 0. As we are multiplying probabilities together, the probability that the entire document occurs in the `not kitchen` class would be 0, which is wrong.\n",
    "\n",
    "To resolve this issue, we use **add one smoothing**. This is adds one to the count of all words in the known vocabulary, meaning the minimum would be 1, not 0. This fixes the previous problem stated as it would not reduce the probability that the entire document occurs in the `not kitchen` class would be 0, leaving a valid probability.\n",
    "The **known vocabulary** is the collection of all the words that occur in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The labels assigned by a Naive Bayes classifier to the sentences contained in the variable `testsentences` above are \\['kitchen','not-kitchen','not-kitchen','not-kitchen','kitchen','not-kitchen','kitchen','not-kitchen','not-kitchen','not-kitchen','kitchen','not-kitchen'\\].  Using this example to illustrate your answer, **explain** how each of the performance metrics of *accuracy*, *precision*, *recall* and *F1-score* are calculated.  Which of these metrics would you use to choose between classification models in this example?  **Justify your answer**. \\[10 marks\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I have printed the classes given to the test sentences. In the question it states that there are meant to be 4 test sentences with the class `kitchen` and 8 with the class `not-kitchen`. The data below conveys that the classifier has identified **3** `kitchen` sentences and **9** `not-kitchen` sentences. This shows that the classifier is not 100% *accurate*, as it has classified atleast 1 test sentence incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "kitchen\n",
      "kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n",
      "not-kitchen\n"
     ]
    }
   ],
   "source": [
    "x = NBClassifier()\n",
    "x.train(trainingsentences)\n",
    "for i in testsentences:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers\n",
    "In order to evaluate classifiers, we can calculate the accuracy, precision, recall and F1-Score. These allow to see how good a classifier would be when classifying  **unseen documents**, this is a label dataset that has not been used in training. To help demonstrate the accuracy and error rate, a confusion matrix can be used. This maps: \n",
    "- True positives **(TP)**: Should be labelled 'kitchen' and was!\n",
    "- True negatives **(TN)**: Should be labelled 'kitchen', but labelled 'not-kitchen'.\n",
    "- False positives **(FP)**: Should have been labelled 'not-kitchen' but labelled 'kitchen'.\n",
    "- False negatives **(FN)**: Should be labelled 'not-kitchen' and was!\n",
    "In addition, the total number of sentences / data items is represented using **N**.\n",
    "\n",
    "The code below is a function that calculates the **accuracy or error rate** of a classifier & prints the **Confusion matrix**, or produces a **classification report**, which consists of the precision, recall, accuracy and f1.score for both classes, `kitchen` and`not kitchen`, by supplying the parameters:\n",
    "\n",
    "1. Accuracy Score\n",
    "2. Error Rate \n",
    "3. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultPrinter(x):\n",
    "    classifier1 = NBClassifier()\n",
    "    classifier1.train(trainingsentences)\n",
    "    predictedClasses = []\n",
    "    classifier1.classify_many(testsentences)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    actualClasses = []\n",
    "    for i in testsentences:\n",
    "        actualClasses.append(i[1])\n",
    "        \n",
    "    results = confusion_matrix(actualClasses, classifier1.classify_many(testsentences))\n",
    "    print('Confusion Matrix:\\n')\n",
    "    print(results, '\\n')\n",
    "        \n",
    "    if (x== 1 ):\n",
    "        print('Accuracy:', accuracy_score(actualClasses, classifier1.classify_many(testsentences)))\n",
    "    if (x== 2 ):\n",
    "        print('Error Rate:', 1 - accuracy_score(actualClasses, classifier1.classify_many(testsentences)))\n",
    "    if (x== 3):\n",
    "        print('Classification Report:\\n\\n', classification_report(actualClasses, classifier1.classify_many(testsentences)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "The accuracy is the proportion of items in a test that are classified correctly.\n",
    "To calculate **accuracy** the forumla is followed:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifies correctly (TP + TN)}}\n",
    "{\\mbox{total number of test documents (N)}}$$\n",
    "\n",
    "For example, if all twelve of the test sentences where classified correctly, then the *accuracy* would be 1. Where as if half of the sentences were classified correctly, then the classifier would have an accuracy of only 0.5. The accuracy of my classifier is calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[1 2]\n",
      " [1 8]] \n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "resultPrinter(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is printed showing the values of TP, FP, FN and TN. We can work out the value of N by adding all of these values together. We can check if the accuracy has been calculated correctly by subsituting the values for TP and TN into the formula above. This accuracy means that the classifier is good at classifying documents correctly. A high accuracy score tends to be desireable.\n",
    "\n",
    "$$\\mbox{Accuracy}=\\frac{1+8}{1+8+2+1} \\qquad \\mbox{=} \\qquad  \\frac{9}{12} \\qquad  \\mbox{or} \\qquad  \\mbox{0.75 (2.d.p)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error rate\n",
    "The error rate is the proportion of items in the test that are classified incorrectly.\n",
    "To calculate the *Error rate*:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifies incorrectly (FP + FN)}}\n",
    "{\\mbox{total number of test documents (N)}}$$\n",
    "\n",
    "This could also be calculated by doing **1 - accuracy**. For example, if all twelve of the test sentences where classified incorrectly, then the *error rate* would be 1. Where as if half of the sentences were classified incorrectly, then the classifier would have an error rate of only 0.5. The error rate of my classifier is calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[1 2]\n",
      " [1 8]] \n",
      "\n",
      "Error Rate: 0.25\n"
     ]
    }
   ],
   "source": [
    "resultPrinter(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation checked by subsituting the values into the formula above. The error rate calculated supports that the classifier is good at classifying documents correctly, since a low error rate is desireable.\n",
    "\n",
    "$$\\mbox{Accuracy}=\\frac{2+1}{1+8+2+1} \\qquad \\mbox{=} \\qquad  \\frac{3}{12} \\qquad  \\mbox{or} \\qquad  \\mbox{0.25 (2.d.p)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "This is the proportion of `kitchen` predictions that are correct, and is calculated with the forumla below:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P=\\frac{TP}{TP + FP}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For example, if 3 of the 4 `kitchen` items are predicted correctly and 2 of the `not-kitchen` are predicted incorrectly, the precision is 3/3+2, which is 0.6.\n",
    "\n",
    "### Recall\n",
    "This is the proportion of actual `kitchen` features/items that were predicted correctly :\n",
    "\\begin{eqnarray*}\n",
    "R=\\frac{TP}{TP + FN}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For example, if three of the four `kitchen` items are predicted correctly the recall is 3/3+1, which is 0.75.\n",
    "\n",
    "### F1-Score\n",
    "\n",
    "We want high **recall & precision**. We combine: precision (P) & recall (R) scores using `F1-Score`.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "F1 = \\frac{2\\times P\\times R}{P+R}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Using the example recall and precision scores from above, the F1-Score would be (2 * 0.6 * 0.75) / (0.6 + 0.75) = 2/3 or 0.66 (2.d.p).\n",
    "\n",
    "The F1-score is the harmonic mean of the precision and recall. This means that it will always *sway* closer to the lower value of the two.\n",
    "\n",
    "The code below prints a copy of the confusion matrix, along with values for the precision, recall and f1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[1 2]\n",
      " [1 8]] \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     kitchen       0.50      0.33      0.40         3\n",
      " not-kitchen       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.65      0.61      0.62        12\n",
      "weighted avg       0.72      0.75      0.73        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultPrinter(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that the precision, recall and f1-score values of class `kitchen` have been correctly computed.\n",
    "\\begin{eqnarray*}\n",
    "P=\\frac{TP}{TP + FP} = \\frac{1}{1 + 1} = 0.5 (2.d.p)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "R=\\frac{TP}{TP + FN} = \\frac{1}{1 + 2} = 0.33 (2.d.p)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "F1 = \\frac{2\\times P\\times R}{P+R} = \\frac{2* 0.5* 0.33}{0.5 + 0.33} = 0.40 (2.d.p)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a value for how accurate a classification model is at identifying whether a sentence is part of the `kitchen` class or not, we should use the **F1-Score**.\n",
    "\n",
    "This is because it gives an harmonic-mean of precision and recall, returning a reliable value. Using the precision/recall score alone,  only takes into account the proportion of correct predictions or the proportion of items predicted correctly, where as we want to take both into account.\n",
    "\n",
    "Accuracy isn't great at comparing the classifiers as it doesnt take into account if a classifier may be accurate at specifying one class correctly, rather than all classes. **E.g.** The results show the classifier coded could be good at predicting the `not-kitchen` class, but not the `kitchen` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Training Data for Sentiment Analysis (25 marks)\n",
    "The objective of this question is to investigate the extent to which performance of a Naive Bayes classifier is affected by the quantity and quality of the training data.  Does more training data mean better performance?  Is performance degraded if we train on one domain and test on another domain?  For example, suppose we train a sentiment classifier on book reviews and then test that classifier on a collection of dvd reviews. Does it perform as well as it would when trained on dvd reviews?\n",
    "\n",
    "The code below is included to enable you to get pre-formatted training and test data for a given category (evenly split between positive and negative reviews).  In this question, there are marks available for the quality of your programming, your experimental design and your interpretation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data)  \n",
    "    n = len(data)  \n",
    "    train_indices = random.sample(range(n), int(n * ratio))          \n",
    "    test_indices = list(set(range(n)) - set(train_indices))    \n",
    "    train = [data[i] for i in train_indices]           \n",
    "    test = [data[i] for i in test_indices]             \n",
    "    return (train, test)                       \n",
    " \n",
    "\n",
    "def feature_extract(review):\n",
    "    \"\"\"\n",
    "    Generate a feature representation for a review\n",
    "    :param review: AmazonReview object\n",
    "    :return: dictionary of Boolean features\n",
    "    \"\"\"\n",
    "    return {word:True for word in review.words()}\n",
    "\n",
    "def get_training_test_data(category,ratio=0.7,seed=candidateno):\n",
    "    \"\"\"\n",
    "    Get training and test data for a given category and ratio, pre-formatted for use with NB classifier\n",
    "    :param category: category of review corpus, one of [\"kitchen, \"dvd, \"book\", \"electronics\"]\n",
    "    :param ratio: proportion of data to use as training data\n",
    "    :return: pair of lists \n",
    "    \"\"\"\n",
    "    reader=AmazonReviewCorpusReader().category(category)\n",
    "    random.seed(candidateno)\n",
    "    pos_train, pos_test = split_data(reader.positive().documents(),ratio=ratio)\n",
    "    neg_train, neg_test = split_data(reader.negative().documents(),ratio=ratio)\n",
    "    train_data=[(feature_extract(review),'P')for review in pos_train]+[(feature_extract(review),'N') for review in neg_train]\n",
    "    test_data=[(feature_extract(review),'P')for review in pos_test]+[(feature_extract(review),'N') for review in neg_test]\n",
    "    return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) By varying the amount of training data used, **investigate** the impact of the amount of training data used on the accuracy of a Naive Bayes classifier for each of the four domains: *dvd*, *book*, *kitchen* and *electronics*.  You should use the NaiveBayesClassifier from the `nltk.classify` library.  You should also use a table and an appropriate graph(s) to display your results.  Make sure you **discuss** your results and conclusions. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Design\n",
    "\n",
    "In order to investigate how the amount of training data impacts the accuracy of a classifier, I have decided test the accuracy at four evenly spaced out ratios of 0.2, 0.4, 0.6 and 0.8. From these range of values, the tests should be able to indicate whether much less training data, i.e. ratio of 0.2, differs much in accuracy compared to a higher ratio of 0.8. For each feature, four accuracies will be calculated, one for each ratio, using the same testing data. These accuracys will be stored in an array, in order from ratio 0.2, to ratio 0.8, so it can be presented in a table and graph clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_training_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-30e5d5a9ae2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0maccs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtrain_data1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_data2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_data3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_training_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating arrays to hold accuracy for each ratio per domain \n",
    "I have chosen to use four different ratios to change the size of the training data for each domain.\n",
    "This takes a while to load, so please be patient\n",
    "'''\n",
    "import nltk.classify.naivebayes\n",
    "import numpy as np\n",
    "NB = nltk.NaiveBayesClassifier\n",
    "dvda = []\n",
    "booka = []\n",
    "kita = []\n",
    "eleca = []\n",
    "features = {\"dvd\", \"book\", \"kitchen\", \"electronics\"}\n",
    "accs = {0.2, 0.4, 0.6, 0.8}\n",
    "for feature in features:\n",
    "        train_data1, testing_data1 = get_training_test_data(feature, 0.2)\n",
    "        train_data2, testing_data1 = get_training_test_data(feature, 0.4)\n",
    "        train_data3, testing_data1 = get_training_test_data(feature, 0.6)\n",
    "        train_data4, testing_data1 = get_training_test_data(feature, 0.8)\n",
    "        accuracy_1 = nltk.classify.accuracy(NB.train(train_data1), testing_data1)\n",
    "        accuracy_2 = nltk.classify.accuracy(NB.train(train_data2), testing_data1)\n",
    "        accuracy_3 = nltk.classify.accuracy(NB.train(train_data3), testing_data1)\n",
    "        accuracy_4 = nltk.classify.accuracy(NB.train(train_data4), testing_data1)\n",
    "        if (feature == \"dvd\"):\n",
    "            dvda.append(accuracy_1)\n",
    "            dvda.append(accuracy_2)\n",
    "            dvda.append(accuracy_3)\n",
    "            dvda.append(accuracy_4)\n",
    "        elif (feature == \"book\"):\n",
    "            booka.append(accuracy_1)\n",
    "            booka.append(accuracy_2)\n",
    "            booka.append(accuracy_3)\n",
    "            booka.append(accuracy_4)\n",
    "        elif (feature == \"kitchen\"):\n",
    "            kita.append(accuracy_1)\n",
    "            kita.append(accuracy_2)\n",
    "            kita.append(accuracy_3)\n",
    "            kita.append(accuracy_4)\n",
    "        elif (feature == \"electronics\"):\n",
    "            eleca.append(accuracy_1)\n",
    "            eleca.append(accuracy_2)\n",
    "            eleca.append(accuracy_3)\n",
    "            eleca.append(accuracy_4)\n",
    "            \n",
    "            \n",
    "data = [['DVD', dvda[0], dvda[1], dvda[2], dvda[3]],\n",
    "        ['Book', booka[0], booka[1], booka[2], booka[3]],\n",
    "        ['Kitchen',kita[0],kita[1],kita[2],kita[3]], \n",
    "        ['Electronics', eleca[0],eleca[1],eleca[2],eleca[3]]]\n",
    "        \n",
    "df = pd.DataFrame(data,columns=['Domain','Accuracy at ratio 0.2', 'Accuracy at ratio 0.4', 'Accuracy at ratio 0.6', 'Accuracy at ratio 0.8'])\n",
    "display(df)\n",
    "\n",
    "barWidth = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "b = np.arange(len(dvda))\n",
    "x1 = ax.bar(b, dvda, barWidth, bottom=0)\n",
    "x2 = ax.bar(b + barWidth, booka, barWidth, bottom=0)\n",
    "x3 = ax.bar(b + barWidth*2, kita, barWidth, bottom=0)\n",
    "x4 = ax.bar(b + barWidth*3, eleca, barWidth, bottom=0)\n",
    "\n",
    "ax.set_title('Impact of amount of training data on accuracy per domain')\n",
    "ax.set_xticks(b + barWidth)\n",
    "ax.set_xticklabels(('0.2', '0.4', '0.6', '0.8'))\n",
    "\n",
    "ax.legend((x1[0], x2[0], x3[0], x4[0]), ('DVD', 'Book', 'Kitchen', 'Electronics'))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.autoscale_view()\n",
    "plt.rcParams[\"figure.figsize\"] = [20,9]\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results discussion\n",
    "\n",
    "**Distrubution of accuracy (DOA)** = **highest accuracy - lowest accuracy**\n",
    "\n",
    "We wish for a small Distribution of accuracy, along with overall high accuracies. This is because we want a classifier that can classify items accurately to all classes/domains.\n",
    "\n",
    "From the table and graph, the results show that by varying the amount of training data, the accuracy of the classifier per class does affect the distrubution of accuracys.\n",
    "\n",
    "For 0.2, there is not much distribution between classes, other than `book` being slightly less accurate than the rest. This means for a small amount of data, the classifier is less accurate for classifying the `book` class. The *DOA* for ratio 0.2 is 0.7675 - 0.7350 = **0.1075**\n",
    "\n",
    "For 0.4,the accuracy of all the classes is high , with only a slight distribution in accuracy. This means for this size of data, the classifier is good at classifying all classes to a high accuracy. This is supported with: The *DOA* for ratio 0.4 is 0.8250 - 0.7950 = **0.03**\n",
    "\n",
    "For ratio 0.6, the accuracy for classes such as kitchen and DVD are very high, meaning this is a good amount of data to give classify items of the class `DVD` and `kitchen`. There is quite a distribution between classes `kitchen` and `book`, meaning that the amount of data is not suitable for classifying items of the class `book`. The *DOA* for ratio 0.6 is 0.8475 - 0.7125 = **0.135**\n",
    "\n",
    "For 0.8, this is the largest amount of training data tested for this classifier. From the graph, this amount of data is a good amount for an accurate classification of  items for all classes, bar `book`. The *DOA* of this amount of data is 0.8100 - 0.7050 = **0.105**.\n",
    "\n",
    "In conclusion, from all the calculated *Distribution of accuracies*, we can see that the ratio of **0.4** is the best amount of training data in order to ensure accurate classifications for **all classes**, along with high accuracies of classification. This supports the claim the amount of data does impact the accuracy of classification, with some amounts of data achieveing much higher accuracies overall than others which give higher accuracies to a single domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For each possible combination of source and target domain, **evaluate** the accuracy of a Naive Bayes classifier trained on the source domain and tested on the target domain.  There are four domains so there are 16 possible combinations you should consider.  You should use a table and an appropriate graph(s) to display your results.  Make sure you **discuss** your results and conclusions. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Design\n",
    "\n",
    "The 16 Combinations are:\n",
    "1. DVD training data with DVD, Book, Kitchen & Electronic testing data\n",
    "2. Book training data with DVD, Book, Kitchen & Electronic testing data\n",
    "3. Kitchen training data with DVD, Book, Kitchen & Electronic testing data\n",
    "4. Electrons training data with DVD, Book, Kitchen & Electronic testing data\n",
    "\n",
    "The code will calculate the accuracy of each combination and store them into four arrays, one for every domain/class.\n",
    "These will then be translated into a table and corrosponding graph to help see how the accuracy is affected by being trained on the source, but tested on the target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.naivebayes\n",
    "import numpy as np\n",
    "NB = nltk.NaiveBayesClassifier\n",
    "dvdar = []\n",
    "bookar = []\n",
    "kitar = []\n",
    "elecar = []\n",
    "\n",
    "dvd_train_data, dvd_testing_data = get_training_test_data(\"dvd\")\n",
    "book_train_data, book_testing_data = get_training_test_data(\"book\")\n",
    "kitchen_train_data, kitchen_testing_data = get_training_test_data(\"kitchen\")\n",
    "electronics_train_data, electronics_testing_data = get_training_test_data(\"electronics\")\n",
    "\n",
    "dvdar.append(nltk.classify.accuracy(NB.train(dvd_train_data), dvd_testing_data))\n",
    "dvdar.append(nltk.classify.accuracy(NB.train(dvd_train_data), book_testing_data))\n",
    "dvdar.append(nltk.classify.accuracy(NB.train(dvd_train_data), kitchen_testing_data))\n",
    "dvdar.append(nltk.classify.accuracy(NB.train(dvd_train_data), electronics_testing_data))\n",
    "\n",
    "bookar.append(nltk.classify.accuracy(NB.train(book_train_data), dvd_testing_data))\n",
    "bookar.append(nltk.classify.accuracy(NB.train(book_train_data), book_testing_data))\n",
    "bookar.append(nltk.classify.accuracy(NB.train(book_train_data), kitchen_testing_data))\n",
    "bookar.append(nltk.classify.accuracy(NB.train(book_train_data), electronics_testing_data))\n",
    "\n",
    "kitar.append(nltk.classify.accuracy(NB.train(kitchen_train_data), dvd_testing_data))\n",
    "kitar.append(nltk.classify.accuracy(NB.train(kitchen_train_data), book_testing_data))\n",
    "kitar.append(nltk.classify.accuracy(NB.train(kitchen_train_data), kitchen_testing_data))\n",
    "kitar.append(nltk.classify.accuracy(NB.train(kitchen_train_data), electronics_testing_data))\n",
    "\n",
    "elecar.append(nltk.classify.accuracy(NB.train(electronics_train_data), dvd_testing_data))\n",
    "elecar.append(nltk.classify.accuracy(NB.train(electronics_train_data), book_testing_data))\n",
    "elecar.append(nltk.classify.accuracy(NB.train(electronics_train_data), kitchen_testing_data))\n",
    "elecar.append(nltk.classify.accuracy(NB.train(electronics_train_data), electronics_testing_data))\n",
    "\n",
    "data = [['DVD', dvdar[0], dvdar[1], dvdar[2], dvdar[3]], ['Book', bookar[0], bookar[1], bookar[2], bookar[3]], ['Kitchen',kitar[0], kitar[1], kitar[2], kitar[3]],  ['Electronics', elecar[0], elecar[1], elecar[2], elecar[3]]]\n",
    "df = pd.DataFrame(data,columns=['Domain','DVD', 'Book', 'Kitchen', 'Electronics'])\n",
    "display(df)\n",
    "\n",
    "barWidth = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "b = np.arange(len(dvdar))\n",
    "x1 = ax.bar(b, dvdar, barWidth, bottom=0)\n",
    "x2 = ax.bar(b + barWidth, bookar, barWidth, bottom=0)\n",
    "x3 = ax.bar(b + barWidth*2, kitar, barWidth, bottom=0)\n",
    "x4 = ax.bar(b + barWidth*3, elecar, barWidth, bottom=0)\n",
    "\n",
    "ax.set_title('Accuracy of a classifier trained on the source domain and tested on the target domain')\n",
    "ax.set_xticks(b + barWidth)\n",
    "ax.set_xticklabels(('DVD', 'Book', 'Kitchen', 'Electronics'))\n",
    "\n",
    "ax.legend((x1[0], x2[0], x3[0], x4[0]), ('DVD', 'Book', 'Kitchen', 'Electronics'))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Source Domain')\n",
    "ax.autoscale_view()\n",
    "plt.rcParams[\"figure.figsize\"] = [20,9]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "\n",
    "As suspected, by training it the source domain, the target domain with the highest accuracy tends to same as the source domain. This is evidently present in source domains: `DVD & Kitchen`. As the data is trained source domain, the highest accuracy should of target domain should be the same as the source domain. Moreover even in source domains such as `Book & Electronics` the corrosponding target domains have the second highest accuracy. \n",
    "\n",
    "What the results further show is that the target domain of `DVD` has a high accuracy of classifying for all of the source domains, meaning that accurate classification of DVD target domains is achieveable on all source domains present. This is not the case for target domains such as `Book` however as much less accurate classification takes place on source domains that are not its own.\n",
    "\n",
    "In conclusion, for some classes such as DVD, the class of the source domain does not matter, as high accuracies are achieveable on all domains, while more frequently, a classifier of *x* source domain will have the highest accuracy of classification for *x* target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Adapt your code so that you can build and use a training set built from multiple categories.  Now **investigate** how having a mixture of source domains affects the accuracy of the Naive Bayes classifier on the target domain.  Make sure you control for or consider how much any improvements are due to the quantity of the training data. \\[9 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Design\n",
    "\n",
    "Creating 4 sets of data, each set with an extra domain than the previous. Each domain will be trained and tested against the different sets. This means each domain will have 4 different values of accuracy, each for a different number of sets.The accuracy values will be stored in arrays, one for each target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.naivebayes\n",
    "import numpy as np\n",
    "NB = nltk.NaiveBayesClassifier\n",
    "\n",
    "s1 = [] # Array of all accuracy values at DVD target domain\n",
    "s2 = [] # Array of all accuracy values at Book target domain\n",
    "s3 = [] # Array of all accuracy values at Kitchen target domain\n",
    "s4 = [] # Array of all accuracy values at Electronics target domain\n",
    "\n",
    "setOf1 = kitchen_train_data\n",
    "setOf2 = dvd_train_data + kitchen_train_data\n",
    "setOf3 = dvd_train_data + kitchen_train_data + electronics_train_data\n",
    "setOf4 = dvd_train_data + kitchen_train_data + electronics_train_data + book_train_data\n",
    "\n",
    "s1.append(nltk.classify.accuracy(NB.train(setOf1), dvd_testing_data))\n",
    "s1.append(nltk.classify.accuracy(NB.train(setOf1), book_testing_data))\n",
    "s1.append(nltk.classify.accuracy(NB.train(setOf1), kitchen_testing_data))\n",
    "s1.append(nltk.classify.accuracy(NB.train(setOf1), electronics_testing_data))\n",
    "\n",
    "s2.append(nltk.classify.accuracy(NB.train(setOf2), dvd_testing_data))\n",
    "s2.append(nltk.classify.accuracy(NB.train(setOf2), book_testing_data))\n",
    "s2.append(nltk.classify.accuracy(NB.train(setOf2), kitchen_testing_data))\n",
    "s2.append(nltk.classify.accuracy(NB.train(setOf2), electronics_testing_data))\n",
    "\n",
    "s3.append(nltk.classify.accuracy(NB.train(setOf3), dvd_testing_data))\n",
    "s3.append(nltk.classify.accuracy(NB.train(setOf3), book_testing_data))\n",
    "s3.append(nltk.classify.accuracy(NB.train(setOf3), kitchen_testing_data))\n",
    "s3.append(nltk.classify.accuracy(NB.train(setOf3), electronics_testing_data))\n",
    "\n",
    "s4.append(nltk.classify.accuracy(NB.train(setOf4), dvd_testing_data))\n",
    "s4.append(nltk.classify.accuracy(NB.train(setOf4), book_testing_data))\n",
    "s4.append(nltk.classify.accuracy(NB.train(setOf4), kitchen_testing_data))\n",
    "s4.append(nltk.classify.accuracy(NB.train(setOf4), electronics_testing_data))\n",
    "\n",
    "\n",
    "data = [['DVD', s1[0], s1[1], s1[2], s1[3]], ['Book', s2[0], s2[1], s2[2], s2[3]], ['Kitchen',s2[0], s2[1], s2[2], s2[3]],  ['Electronics', s3[0], s3[1], s3[2], s3[3]]]\n",
    "df = pd.DataFrame(data,columns=['Domain','1 Set', '2 Sets', '3 Sets', '4 Sets'])\n",
    "display(df)\n",
    "\n",
    "barWidth = 0.2\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "b = np.arange(len(s1))\n",
    "x1 = ax.bar(b, s1, barWidth, bottom=0)\n",
    "x2 = ax.bar(b + barWidth, s2, barWidth, bottom=0)\n",
    "x3 = ax.bar(b + barWidth*2, s3, barWidth, bottom=0)\n",
    "x4 = ax.bar(b + barWidth*3, s4, barWidth, bottom=0)\n",
    "\n",
    "ax.set_title('How a mixture of source domains affects the accuracy of a classifier')\n",
    "ax.set_xticks(b + barWidth)\n",
    "ax.set_xticklabels(('1', '2', '3', '4'))\n",
    "\n",
    "ax.legend((x1[0], x2[0], x3[0], x4[0]), ('DVD', 'Book', 'Kitchen', 'Electronics'))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Number of Source Domains')\n",
    "ax.autoscale_view()\n",
    "plt.rcParams[\"figure.figsize\"] = [20,9]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of results\n",
    "\n",
    "By increasing the number of source domains, the accuracy of the classification on target domains increases. This is conveyed for target domain of any of the domains. All accuracies increase from having one source domains to four, meaning that using kitchen training data as a base source domain (a part of all sets of domains), this is the case. Here i have only shown 1 possibility, as there are 4^4 combinations (256) but i have only shown 1. Another interesting point to make is that for 3 source domains, the accuracy for each target domain is much higher. This would suggest that the combination of the three sets allows for accurate classification of all target domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to verify that the length of your submission does not exceed 2000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 1282\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "filepath=\"a1.ipynb\"\n",
    "question_count=1282\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
