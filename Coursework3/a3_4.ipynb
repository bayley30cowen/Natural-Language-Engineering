{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 3: Question 4\n",
    "\n",
    "For this assessment, you are expected to complete and submit 4 notebook files.  There is 1 notebook file for each question (to speed up load times).  This is notebook 4 out of 4.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Question Answering (25 marks)\n",
    "Imagine that you have access to a database containing English-language newspaper and magazine articles for the last 50 years. **Describe in detail** how you would go about building a question-answering system to answer questions about historical events including:\n",
    "* Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?\n",
    "* In which years has the football World Cup been held in France?\n",
    "* Where did Princess Diana die?\n",
    "\n",
    "You are not expected to write any code for this question.  You should, however, describe the challenges and the strategies your system will employ to attempt to overcome them.\n",
    "\n",
    "As no code is expected, you can, if you wish, submit a pdf rather than a notebook file for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factoid questions are questions that answered with simple facts expressed in shorts texts or passages. Many question, not all, are concerned with relationships between entities, an example of this could be 'when was X born?' By identifiying these named entities, allows for these questions to be answered. There are two paradigms of answering a question, which are `Information Retrieval based` and `Knowledge based`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Retrieval Based (IR-Based)**<br><br>\n",
    "These techniques are used to find **relevant documents** and passages (also known as document retrieval) and **reading comprehension** algorithms to draw an answer from related spans of text.\n",
    "Information Retrieval is when we obtain information resources relevant to the *information need* from a large collection of information resources. An example of a large collection for example could be the web.\n",
    "*Information need* is expressed as a **Query** and can be:\n",
    "- A set of query terms (i.e. set of words/phrases\n",
    "- An expression in some special purpose *query language*\n",
    "- An expression in natural langauge.\n",
    "\n",
    "`A Query is` - A collection of terms connected by logical operations\n",
    "\n",
    "The issues with information retrieval are `accuracy`, `efficiency` and `relevance ranking`. Accuracy is ensuring the correct result is found. Efficiency is finding the expected result in a reasonable amount of time, where this issue is especially prominent for web-scale Information Retrieval. Relevance ranking is presenting users with information in a manageable manner. On search engines like *Google*, the most relevant information is displayed to the user on page 1, rarely requiring users to move onto page 2, 3 etc. By presenting the information in a structured and manageable way, allows for ease of information retrieval.\n",
    "\n",
    "The goal of information retrieval is to find relevant documents, but what is classifies a document as relevant?\n",
    "A relevant document can be documents containing the given query terms. Relevant documents can also be documents satisfying the query expression. The retrieved documents can be ranked by relevance, as in `Ranked retrieval`, or not, as in `Boolean retrieval`. Boolean is when there is either two possibilities i.e. the documents are found, or they arent.\n",
    "\n",
    "**Boolean Information Retrieval**<br><br>\n",
    "The document is presented as a set of *keywords*. The *query* is expressed as a Boolean combination of the keywords. The keywords are connected by logical operators such as `AND, OR and NOT`. Brackets are also used to indicate the scope of these Boolean combinations. A basic example relating one of the above questions could be:\n",
    "`(football AND (world OR france))`<br>\n",
    "The output of this query would consist of a set of relevant documents, but with no partial matching and no ranking. Each term within the query and the document will have `Standard Canonicalisation of Terms` applied to them. This is a technique to reduce the term vocabulary, overall improving efficiency of the retrieval, and often (but not always) improving the performace of the retrieval, this is because there may possibly be more overlaps. The standard canonicalisation of terms consists of:\n",
    "- Case normalisation\n",
    "- Number normalisation\n",
    "- Removal of stopwords\n",
    "- Stemming / Lemmatisation\n",
    "\n",
    "When searching for relevant documents, we need to know which documents contain the query terms. For example, when looking for the answer to the question *`Where did Princess Diana die?`*, we would want to know which documents contain words such as **Princess** or **Diana**. \n",
    "\n",
    "*Measuring Performance* can be done through calculating precision, recall and the F1 Score. Precision (P) is calculated by dividing the number of overlaps (True Positives), divided by all the retrived documents (TP + FP). This is a way of checking how accurate the document retrieval is. Recall (R) is the number of overlaps divided by the number of all relevant documents (TP + FN). Recall is used to show how many documents have been returned, which is used to help calculate the efficiency. It can be difficult to measure recall in some case (when large collections such as the web is used, since we wont know the amount of False Negatives (the number of relevant documents overall). F1 score shows the balance between precision and recall, as one increases i.e. precision gets better, the recall will worsen.\n",
    "![Precision, Recall & F1 Equations](prerecall.png)\n",
    "<br><br>\n",
    "\n",
    "`Indexing documnts` is used as we need to know which documents contain the query terms, It can be very inefficient especially on large collections, to search all documents every time for each term. This problem is avoided through the use of an `inverted index`. This in an index of documents where docs are indexed by the *potential query terms* that they contain. An example can be shown below:\n",
    "![Inverted Index example](index.png)<br><br>\n",
    "Retrieval using inverted index: \n",
    "- Start with one posting list and compute documents in the intersection (AND operator) or the Union (OR operator) successfully with other lists.\n",
    "- Return documents are all posting lists have been considered\n",
    "\n",
    "The two lists are compared with pointers. If performing and AND operation within the query i.e. (football `AND` ...) The pointers will go down the lists and if two of the posting lists point at the same document, it is added to a `merged list` i.e. the overlap between terms in the query. If more than two lists are being queried, then the process will repeat with each list in term (if there are lots of lists this will decreases efficiency, but it will be most efficient to start with the shortest posting lists since they will create the smaller of the merge lists). The algoritms complexity is linear in the total length of postal lists the longer the lists, and the more of them, the less efficient the algorithm becomes. In addition algorithms for NOT and OR operators are analgous to AND algoritm talked about above.\n",
    "\n",
    "Overall Boolean retrieval would have a very high recall as it will return all documents that are found. This suggests a low precision, when compared to the other retrieval method.\n",
    "\n",
    "**Ranked Retrieval**\n",
    "\n",
    "Boolean retireval discussed above is often useless, since it returns too many documents, and the documents are not ranked in relevance. For example, we may find many documents which contain the word `World`, but many of those documents may be irrelevant to the term `World Cup`. The relevance of a document would increase when less common query terms occur more frequently in the document.\n",
    "\n",
    "`Relevance Scoring` : calculating the relevancy scoring of a document, d, and query, q, we can calculate the tf-idf values. Once a set of all possible relevant documents are obtained, we score them:\n",
    "- A sum of the tf-idf values - tend to be used for OR query types\n",
    "- Product of tf-idf values - tend to be used for AND queries\n",
    "- The **cosine** of the vector representations of d with q.\n",
    "\n",
    "The equations are shown below in order as stated:\n",
    "![relevance scoring equations](tfidf.png)<br><br>\n",
    "\n",
    "**Query expansion**\n",
    "\n",
    "Exact matches between words in documents and queries is NOT necessary. Normalisation is used to ensure matches between docs and queries that are very similar. This includes methods of matching docs & queries which may include:\n",
    "- Spelling Errors\n",
    "- Spelling Alternatives (Spelling variations)\n",
    "- Synonyms (Semantic Variation)\n",
    "- named entity recognition (variation & ambiguity)\n",
    "- and many more.\n",
    "\n",
    "We also use a thesaurus to include the nearest neighbours as alternatives in queries. In practice, this can increase the recall, but can reduce the precision. <br>\n",
    "\n",
    "- An example of where this would be useful would be if a document had the answer to the question `In which years has the football World Cup been held in France?` but the phrase `France` was in all lower case. Without case normalisation, this document would not be used since the exact phrase `France` does not occur in the document. by not implementing these expansions it reduces the reliability of the answer produced as it has ignored a document with the correct answer.\n",
    "- Another example using the quesiton `Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?` is if the document contained the word **Boss** or **President** or any semantic variation of the phrase `Leader`. Since the phrase leader is not within this document, it would be ignored.\n",
    "- Again, if there was a spelling error in a document containing the keyword `Diana` relating to the question `Where did Princess Diana die?`, the document would not be considered if only exact matching was being used.\n",
    "With the question 'Where did Princess Diana die?', the question can further be seen as ambigious through the word `Where`. This is because it could mean where in the world, i.e. Paris, France or it could be literally in which building, *University Hospitals Pitié Salpêtrière - Charles Foix, Paris, France*. Ambiguity and Variation are a big problem faced when using NER (Named Entity Recognition). \n",
    "- A final example related to the `In which years has the football World Cup been held in France?` question could be Name Alternatives due to the phrase `football`, but in America it is called Soccer. Spelling Alternatives would also be present in documents written in American English and British English.\n",
    "\n",
    "Below is a visual representation of how information retrieval works: <br>\n",
    "![Info Retrieval Diagram](inforet.png)<br><br>\n",
    "**Question Processing**\n",
    "- Detects question type, answer type, focus and relations.\n",
    "- Further helps to formualte queries to send to search engine\n",
    "\n",
    "This is the things to extract from the question. The `Anser type detection` decides the `named entity type`, for example person, place etc, of the answer. For the question:\n",
    "- **Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?** : answer type could be a person\n",
    "- **In which years has the football World Cup been held in France?:** answer type will be a list of years (if there is atleast 1 or more years)\n",
    "- **Where did Princess Diana die?**: An answer type of a location or a geo-political entity. \n",
    "\n",
    "The query is formulated by choosing **keywords** for the IR (information retrieval) system. For example within the question `In which years has the football World Cup been held in France?` the keywords could likely consist of: Years, football, World Cup, held, France. \n",
    "\n",
    "In addition some systems may use question type classification. Defines what kind of question has been asked i.e. a definition question, a mathematical questions,a list question etc. The question `In which years has the football World Cup been held in France?` would be classified as a list question, since it wants a list of years which France held the world cup.\n",
    "\n",
    "Other expansions such as `Focus detection` and `Relation extraction` can be used. Focus detection finds the question words that are replaced by the answer i.e. in the question **Where did Princess Diana die?**, the focus words are `Where`, `Princess Diana`, `died`. Relation extraction is finding relationships between entities within the question. In the question In which years has the football World Cup been held in France?, the relationship is **World Cup been `held` in France?**\n",
    "\n",
    "This is called question processing, and the answers had been devised from examples of Jurafsky and Martin.\n",
    "\n",
    "**Question Type Taxonomy**\n",
    "\n",
    "There are 6 Coarse (Core) Classes for answer types:\n",
    "1. ABBREVIATION\n",
    "2. ENTITY\n",
    "3. DESCRIPTION\n",
    "4. HUMAN\n",
    "5. LOCATION\n",
    "6. NUMERIC\n",
    "\n",
    "Each class is then further broken down into finer classes, for example LOCATION can be broken down into city, country, state etc. Question type detection is done either through:\n",
    "1. Hand written Rules\n",
    "2. Machine Learing - most often used\n",
    "3. Hybrid of the two\n",
    "\n",
    "Rules can include the use of regular expressions i.e **Who {is/was/are/were/..} PERSON**, or using other rules called `headword rules`, which is the headword of the first NP (Noun phrase) after Wh- word. An example of this Which `city` in China ....., here `city` is the headword.\n",
    "\n",
    "When identifying the answer type, we often treat the problem as machine learning classificatio. this is done by **defining** a taxonomy of question types. We then **annotate** the training data for each question type then **train** classifiers for each question class.\n",
    "\n",
    "Some features for answer type detection include: question words and phrases, part of speech tags, parse features (headwords), named entities and semantically related words.\n",
    "\n",
    "When trying to find the keywords in a query or document, we need a `keyword selection algorithm`.\n",
    "There are ten steps to the `keyword selection algorithm`:\n",
    "\n",
    "1. Select all non-stop words in quotations\n",
    "2. Select all NNP words in recognised named entities\n",
    "3. Select all complex nominals with their adjectival modifiers\n",
    "4. Select all other complex nominals\n",
    "5. Select all nouns with their adjectival modifiers\n",
    "6. Select all other nouns\n",
    "7. Select all verbs\n",
    "8. Select all adverbs\n",
    "9. Select the QFW (Question Focused Words) (skipped in all previous steps)\n",
    "10. Select all other words\n",
    "\n",
    "**Passage Retrieval**\n",
    "\n",
    "There are three steps to passage retrieval.\n",
    "\n",
    "Step 1. IR engine retrieves documents using query terms<br>\n",
    "Step 2. Segment the documents into shorter units, like paragraphs<br>\n",
    "Step 3. Passage ranking, this is done using the answer type.<br>\n",
    "\n",
    "Some features of passage retrieval, using either rule based classifiers or supervised machine learning, include:\n",
    "1. Number of named entities of the right type in a passage\n",
    "2. Number of query words in passage\n",
    "3. Number of question N-grams also in passage\n",
    "4. Proximity of query keywords to each other in passage\n",
    "5. Longest sequence of question words\n",
    "6. Rank of the document containing passage\n",
    "\n",
    "An N=gram is a number of words that appear next to eachother in sequence, and whether they occur next to eachother within the passage. An example of this could be `Wolrd Cup`. \n",
    "\n",
    "**Answer Extraction**\n",
    "\n",
    "Run an answer type tagger on the passages, each answer type requires a name-entity type that detects it, An example of an entity type could be CITY, the tagger has the tag CITY. This can be a full NER (Named Entity Relation), simple expression or a hybrid of the two, where it returns the string of the correct type.\n",
    "E.g. **Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?** `(PERSON)` returning the  answer 'Arthur Scargill' who is of type PERSON.\n",
    "\n",
    "**Ranking Candidate Answers**\n",
    "\n",
    "But what about if there was multiple candidate answers returned, i.e. of the same type. Then machine learning features for ranking candaiate answers are used. An example of this for the question `Where did Princess Diana die?` could return the passage **Diana, Princess of `Wales` died in hospital after being injured in a car crash in a road tunnel in Paris, `France`**. Here two candidate answers are returned, but we know the answer should be France. A feature such as keyword distance, using keyword `crash` can be used here to give the more likely of France.\n",
    "\n",
    "List of machine learning features:\n",
    "1. *answer type match* : when the candidate contains a phrase with the correct answer type\n",
    "2. *pattern match* : the candidate matches the regular expression pattern\n",
    "3. *question keywords*: the number of question keywords in the candidate\n",
    "4. *keyword distance* : distance in words between the candidate and query words\n",
    "5. *novelty factor* : a word in the candidate which is not in the query\n",
    "6. *apposition features* : the candidate is an appositive to question terms\n",
    "7. *punctuation location* : the candidate is immediately followed by a punctuation mark\n",
    "8. *sequence of question terms* : the length of the longest sequence of question terms that occurs in the candidate answer\n",
    "\n",
    "**Common Evaluation metrics**\n",
    "1. Accuracy - does the answer match the gold-labelled answers?\n",
    "2. Mean Reciprocal Rank\n",
    "\n",
    "Accuracy Example: For example, if the answer produced for the question `In which years has the football World Cup been held in France?` was 2006, the accuracy of the candidate answer is *not good* as the gold-labelled answer is 1938 and 1998.\n",
    "\n",
    "**Mean Reciprocal Rank**\n",
    "\n",
    "For each query, return a ranked list of M candidate answers. The query score is $\\frac{1}{Rank of the first correct answer}$\n",
    "If the first answer is correct, the score is 1\n",
    "If second answer is correct, the score is $\\frac{1}{2}$\n",
    "If third answer is correct, the score is $\\frac{1}{3}$\n",
    "and so on.\n",
    "If there are no correct answers, the score is 0\n",
    "\n",
    "The equation is shown below:\n",
    "$$MRR=\\sum_{i=1}^{N} \\frac{1}{Rank_i}$$\n",
    "\n",
    "MMR Example: For the question `Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?` we could have the following answers to the following queries:\n",
    "Query 1: National Union of ..\n",
    "\n",
    "**Results**: Students, Teachers, Journalists, `Miners`, .. (Rank 4)\n",
    "\n",
    "Query 2: Miners Strike of ..\n",
    "\n",
    "**Results**: 1912, `1984`, 1974, ...(Rank 2)\n",
    "\n",
    "`MMR = (1/2 + 1/4) / 2 = 3/8 or (0.375)`\n",
    "\n",
    "\n",
    "**Knowledge Based**\n",
    "\n",
    "This is when we build a semantic or logical representation of the query. We would have a database of facts and we query this database using logical meaning representations. Moreover, Information extraction techniques may be used to build or update a database. We start by building a semantic representation of the query: Times, dates, location, entities, numeric quantities etc. We then map to query structured data or resources such as:\n",
    "\n",
    "- Geospatial databases\n",
    "- Ontologies\n",
    "- Restaurant review sources / reservation services\n",
    "- Scientific databases\n",
    "\n",
    "\n",
    "**Rule Based Methods**\n",
    "\n",
    "We can write hand-written rules for frequent relations. E.g. for the question `Where did Princess Diana die?` we could write a pattern that would search the question for the word `Where`, a main verb like `die` and it would extract the named entity argument, in this case `Princess Diana`.\n",
    "\n",
    "\n",
    "**Supervised Methods**\n",
    "\n",
    "We may have supervised data which would be a set of questions paired with their answer form. E.g.\n",
    "`Who was the leader of the National Union of Miners during the miners' strikes of the 1980s?`\n",
    "    **leader(National Union of Miners, 1980's)**\n",
    "`In which years has the football World Cup been held in France`\n",
    "    **years(France, World Cup)**\n",
    "`Where did Princess Diana die?`\n",
    "    **location(Princess Diana, ?x)**.\n",
    "    \n",
    "This is then used to map new questions to their correct format. \n",
    "One way of doing this is by parsing the questions then aligning the parse trees to the correct format. A supervised system would parse each pair in the training data and create a bigger set of specific rules allowing it to identify the correct answer format for questions, such as `Where did X die?` to the location relation.\n",
    "The more complex questions, the more complex rules.\n",
    "\n",
    "**Semi-supervised Methods**\n",
    "\n",
    "Even simple questions can take a wide variety of forms, which makes it difficult to create a supervised data set. This is why knowledge based structures have to make use of textual redundancy. Using web texts is a easy way of aligning strings to produce answers. For example a knowledge based approach may use a web text such as `Wikipedia` to identify strings `Princess Diana`, `died in`, `France`. Once these strings have been aligned with the web text, we create new relations that can be queried. To align the strings, we start by using entity linking to link a string such as `Princess Diana` with a web text such as a Wikipedia page.\n",
    "\n",
    "**Hybrid Approaches**\n",
    "We can also approach the problem using a hybrid. Wwe begin by building a shallow semantic representation of the query and then generate answer candidates. The each candidate is scored using richer knowledge sources, such as those used in knowledge based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
